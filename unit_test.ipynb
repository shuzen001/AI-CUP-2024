{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/NLP/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pdfplumber  # 用於從PDF文件中提取文字的工具\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import jieba  # 用於中文文本分詞\n",
    "import pdfplumber  # 用於從PDF文件中提取文字的工具\n",
    "from rank_bm25 import BM25Okapi  # 使用BM25演算法進行文件檢索\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取單個PDF文件並返回其文本內容\n",
    "def read_pdf(pdf_loc, page_infos: list = None):\n",
    "    pdf = pdfplumber.open(pdf_loc)  # 打開指定的PDF文件\n",
    "\n",
    "    # TODO: 可自行用其他方法讀入資料，或是對pdf中多模態資料（表格,圖片等）進行處理\n",
    "\n",
    "    # 如果指定了頁面範圍，則只提取該範圍的頁面，否則提取所有頁面\n",
    "    pages = pdf.pages[page_infos[0]:page_infos[1]] if page_infos else pdf.pages\n",
    "    pdf_text = ''\n",
    "    for _, page in enumerate(pages):  # 迴圈遍歷每一頁\n",
    "        text = page.extract_text()  # 提取頁面的文本內容\n",
    "        if text:\n",
    "            pdf_text += text\n",
    "    pdf.close()  # 關閉PDF文件\n",
    "\n",
    "    return pdf_text  # 返回萃取出的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 643/643 [00:08<00:00, 79.28it/s] \n",
      "100%|██████████| 1035/1035 [00:41<00:00, 24.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_category_corpus(source_path, source_ids):\n",
    "    \"\"\"\n",
    "    只讀取指定 source_ids 中的文件，以減少不必要的讀取。\n",
    "    :param source_path: 資料夾路徑\n",
    "    :param source_ids: 要讀取的文件 id 列表\n",
    "    :return: 該類別對應的 corpus 字典\n",
    "    \"\"\"\n",
    "    corpus_dict = {}\n",
    "    for file in tqdm(os.listdir(source_path)):\n",
    "        file_id = int(file.replace('.pdf', ''))\n",
    "        if file_id in source_ids:  # 僅載入在 source_ids 中的文件\n",
    "            file_path = os.path.join(source_path, file)\n",
    "            corpus_dict[file_id] = read_pdf(file_path)\n",
    "    return corpus_dict\n",
    "\n",
    "def get_unique_source_ids(qs_ref, category):\n",
    "    \"\"\"\n",
    "    根據指定的 category，獲取所有 unique source ids。\n",
    "    :param qs_ref: 問題的 JSON 資料\n",
    "    :param category: 類別名稱 (如 'insurance', 'finance')\n",
    "    :return: 該類別下的 unique source ids 集合\n",
    "    \"\"\"\n",
    "    source_ids = set()\n",
    "    for question in qs_ref[\"questions\"]:\n",
    "        if question[\"category\"] == category:\n",
    "            source_ids.update(question[\"source\"])\n",
    "    return sorted(source_ids)\n",
    "\n",
    "# 使用範例\n",
    "with open('競賽資料集/dataset/preliminary/questions_example.json', 'rb') as f:\n",
    "    qs_ref = json.load(f)\n",
    "\n",
    "source_path_insurance = os.path.join('競賽資料集/reference', 'insurance')\n",
    "insurance_source_ids = get_unique_source_ids(qs_ref, 'insurance')\n",
    "corpus_dict_insurance = load_category_corpus(source_path_insurance, insurance_source_ids)\n",
    "\n",
    "source_path_finance = os.path.join('競賽資料集/reference', 'finance')\n",
    "finance_source_ids = get_unique_source_ids(qs_ref, 'finance')\n",
    "corpus_dict_finance = load_category_corpus(source_path_finance, finance_source_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('BAAI/bge-m3', device='cuda') # 使用BGE模型 m3 AP 0.77\n",
    "x  = ['hello world', 'goodbye world']\n",
    "embeddings = model.encode(x)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 296766.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7796663641929626]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n",
    "\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "def get_embeddings(text_list):\n",
    "    model = SentenceTransformer('BAAI/bge-m3', device='cuda') # 使用BGE模型 m3 AP 0.77\n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to('cuda') for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "corpus = [\n",
    "    \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n",
    "]\n",
    "query = [\"What is BGE M3?\"]\n",
    "\n",
    "pair = [[i,j] for i in query for j in corpus]\n",
    "# passage_embeddings = model.encode(\n",
    "#     corpus, return_dense=True, return_sparse=True, return_colbert_vecs=True\n",
    "# )\n",
    "print(model.compute_score(pair)['colbert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00011137, -0.06657315, -0.00018456, ...,  0.04317538,\n",
       "        -0.02131795,  0.01383029]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_embeddings['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/NLP/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def Bge_retrieve(qs, source, corpus_dict, category):\n",
    "    # 將文檔內容列出來以文字形式\n",
    "    filtered_corpus = [corpus_dict[int(file)] for file in source]\n",
    "    #將文字內容轉為向量 embeddings\n",
    "    model = SentenceTransformer('BAAI/bge-m3', device='cuda') # 使用BGE模型 m3 AP 0.77\n",
    "    corpus_embeddings = model.encode(filtered_corpus, prompt=f'以下是關於{category}的文章', convert_to_tensor=True, device='cuda' )\n",
    "    query_embedding = model.encode(qs,prompt=\"為這個句子生成表示，用以檢索相似的文章: \" ,convert_to_tensor=True, device='cuda')\n",
    "\n",
    "    # 計算相似度\n",
    "    similarities = torch.nn.functional.cosine_similarity(query_embedding, corpus_embeddings)\n",
    "    max_sim_index = similarities.argmax().item()\n",
    "    \n",
    "    return source[max_sim_index]\n",
    "\n",
    "answer_dict = {\"answers\": []}  # 初始化字典\n",
    "with open(os.path.join(\"競賽資料集/reference\", 'faq/pid_map_content.json'), 'rb') as f_s:\n",
    "    key_to_source_dict = json.load(f_s)  # 讀取參考資料文件\n",
    "    key_to_source_dict = {int(key): value for key, value in key_to_source_dict.items()}\n",
    "\n",
    "for q_dict in qs_ref['questions']:\n",
    "    if q_dict['category'] == 'finance':\n",
    "        # 進行檢索\n",
    "        retrieved = Bge_retrieve(q_dict['query'], q_dict['source'], corpus_dict_finance, \"公開資訊觀測站上的上市公司財務報告\")\n",
    "        # 將結果加入字典\n",
    "        answer_dict['answers'].append({\"qid\": q_dict['qid'], \"retrieve\": retrieved})\n",
    "\n",
    "    elif q_dict['category'] == 'insurance':\n",
    "        retrieved = Bge_retrieve(q_dict['query'], q_dict['source'], corpus_dict_insurance, \"⽟⼭銀⾏代銷的保險產品之保單條款\" )\n",
    "        answer_dict['answers'].append({\"qid\": q_dict['qid'], \"retrieve\": retrieved})\n",
    "\n",
    "    elif q_dict['category'] == 'faq':\n",
    "        corpus_dict_faq = {key: str(value) for key, value in key_to_source_dict.items() if key in q_dict['source']}\n",
    "        retrieved = Bge_retrieve(q_dict['query'], q_dict['source'], corpus_dict_faq, \"⽟⼭銀⾏官⽅網站上的常⾒問題\")\n",
    "        answer_dict['answers'].append({\"qid\": q_dict['qid'], \"retrieve\": retrieved})\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Something went wrong\")  # 如果過程有問題，拋出錯誤\n",
    "\n",
    "# 將答案字典保存為json文件\n",
    "with open('result/bge_add_cat.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(answer_dict, f, ensure_ascii=False, indent=4)  # 儲存檔案，確保格式和非ASCII字符\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8213, 0.7396, 0.7956, 0.7409], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "source = ['1001', '1002', '1003', '1004']\n",
    "corpus = ['this is a dog', 'this is a cat', 'this is a puppy', 'this is a kitten']\n",
    "model = SentenceTransformer('BAAI/bge-m3', device='cuda')\n",
    "query = \"這是一隻狗的英文怎麼說\"\n",
    "corpus_embeddings = model.encode(corpus,convert_to_tensor=True, device='cuda')\n",
    "query_embedding = model.encode(query,convert_to_tensor=True, device='cuda')\n",
    "similarities = torch.nn.functional.cosine_similarity(query_embedding, corpus_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 最少的數量為: 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('競賽資料集/dataset/preliminary/questions_example.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# 找出最小的 source 陣列長度\n",
    "min_source_count = min(len(question[\"source\"]) for question in data[\"questions\"])\n",
    "\n",
    "print(f\"Source 最少的數量為: {min_source_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/NLP/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings: tensor([[ 0.0015,  0.0165, -0.0281,  ..., -0.0309,  0.0297, -0.0327],\n",
      "        [ 0.0151,  0.0041, -0.0157,  ..., -0.0281,  0.0408, -0.0251]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [\"样例数据-1\", \"样例数据-2\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "model.eval()\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "# normalize embeddings\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\", sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8791519999504089"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sentence_embeddings[0] @ sentence_embeddings[1].T\n",
    "x.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
